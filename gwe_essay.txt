750-1500 words

Your task in this assignment is to introduce the topic of crowdsourcing science to your readers and inform them about the central issues.

When Jeff Howe first coined the term "crowd-sourcing" in a 2006 article for **Wired** magazine, there was already growing excitment for what seemed like a promising new paradigm. Citing the successes of a user-contributed stock photo company, reality TV shows, and R&D by home scientists, Howe presented crowdsourcing as the democratic brother to outsourcing, where the laborer, self-selecting, was welcome regardless of not only nationality but professional background. Declaring a new era, he welcomed the reader into "the age of the crowd".

Still, Howe was careful to point out potential and existing challenges for the emerging labor model. It was a whole new labor pool, and with it came new rules that he claimed common to crowdsource contributors regardless of task and field. His description of the Crowd is Hobbesian, with "a short attention span", producing "mostly crap", although in the end it's worth it, because the Crowd always manages to "find the best stuff" in the end.

Although dense with hyperbole, Howe wasn't far off in his early judgement of the nature of crowdsourcing. Nearly a decade later, while crowdsourcing techniques continue to gain momentum in a variety fields, the fundamental concerns remain the same, with additional ones arising from the changing terrain of technology along with the emergence of new fields, such as the rise of digital humantiies.

It is the diversity of the crowd-- that which gives crowdsourcing projects the ability to shine where traditional approaches have failed (as Howe notes, the odds of a contributor solving a problem actually increases when they have no formal training in that area)-- that also makes both the process and the product hard to judge. Because contributors can come from a wide variety of backgrounds and skillsets, they are often united only by a focused interest in the subject or activity area, whether amateur or professional. Beyond that, motivations vary by particpant-- whether personal or altruistic, extrinsic or intrinsic. In fact, the term crowdsourcing itself can be misleading-- most crowdsourcing projects involve not crowds, which are defined as large anonymous masses of people, but call on the participation of those members of the public with a particular interest and engagement in the task at hand (Dunn and Hedges).

One way to gain insight into the user is to look into the nature of the task itself. In "Crowdsourcing Systems on the World-Wide Web", a 2011 survey paper evaluating the state of crowdsourcing systems (CS) at that time, the author proposes four basic roles in any CS system: the **slave**, **perspective provider**, **content provider**, and **component provider**. The slave works on problems that need many resources but can be divided into simple subtasks per user, such as in Amazon Mechanical Turk; the perspective provider provides opinions (as in book reviews), the content provider produces her own material, whether artistic or scientific, and the compenent providers create a community or network. However, humans can play multiple roles in a single CS system (Dohan et al.).

Attempts to formulate typologies, or the marking of a system into essential parts and processes, have been prevalent in the research of crowdsourcing and similiar activities. Most specifities thus far have focused on the realm of **crowd science**, also known as **citizen science**, **networked science**, or **massively-collaborative science** (naming proves often to be a science within itself). Crowd science is marked by open participation and the transparency of intermediary steps, to varying degrees (Franzoni and Sauermann).

One popular typology for crowd science, presented by Bonney et al., creates three tiers of projects, distinguished by the level of control contributors have in the overall design. These divisions are **contributory**, **collaborative**, and **co-created projects** (Dunn and Hedges). This task-based approach is useful in defining crowd science, as the classification of the project in turn helps classify its users, which remain the core of all crowdsourcing-related problems. The four fundamental challenges of crowdsourcing systems, as detailed by Dohan et al., are all focused on the care and keeping of the user: how to recruit the user, how to retain the user, how to combine the works of different users, and how to evaluate these user contributions. Even in the most unexpected of use-cases, such as when Jerome Lewis, a social anthropologist worked with Rwandan pygmy hunter-gatherers to target poachers and loggers with specially designed smartphones, defining the type of task makes data gathering and collaboration easier (Gura).

These typologies for crowd science are helpful, but must also be adjusted and overwritten for other areas of academia. With the advent of digital research and the digital humanities, the next challenge is to categorize crowdsourcing methods outside of scientific applications. One particular issue lies in the fact that task and process types are far more difficult and also significant for projects in the humanities; since there is so much diveresity in research material (Dunn and Hedges). Dunn and Hedges propose a research infrastructure for the humanitries that takes into account six types of tasks: those that are mechanical, configurational, editorial, synthetic, investigative and creative, in addition to the asset type and the output type. Previous attempts in the Galleries, Libraries, Archives, and Museums (GLAM) sectors to create typologies focus only on task typologies and do not fully reflect the complexity of the project at hand (Dunn and Hedges).


Regardless of field filtering data issues
Still even with typologies, the question of filtering remains-- and effeciency

Laborers/division of labor X
Typologies help us understand them X
Crowd Science X
How about digital humanities?ardest gathering/filtering --> inefficient?
Pitfalls of both crowd science (everything from publication to stuff-- its listed in one of the articles)
special issues for digihums

Remains extremely attractive to policy makers et al (high returns)


for crowd science:
Gist: crowd science can help, but comes with complications
What is end goal? outreach, or something more?

Cons: 
how to organize/collect data
ensure data sound?
hard to measure growth
Actually enormously expensive in terms of time and effort, managing people and DBs
people management esp. hard -- don’t know who is on other end (amateur/professional)
Hard to guarantee quality of data
Large scale = even harder to control quality
science also has to be “romantic”-- appealing, engaging
have to understand volunteer’s motivation

Challenge of getting citizen-science data through peer review (but getting easier/more common)

