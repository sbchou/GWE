750-1500 words

Your task in this assignment is to introduce the topic of crowdsourcing science to your readers and inform them about the central issues.

When Jeff Howe first coined the term "crowd-sourcing" in a 2006 article for **Wired** magazine, there was already growing excitment for what seemed like a promising new paradigm. Citing the successes of a user-contributed stock photo company, reality TV shows, and R&D by home scientists, Howe presented crowdsourcing as the democratic brother to outsourcing, where the laborer, self-selecting, was welcome regardless of not only nationality but professional background. Declaring a new era, he welcomed the reader into "the age of the crowd".

Still, Howe was careful to point out potential and existing challenges for the emerging labor model. It was a whole new labor pool, and with it came new rules that he claimed common to crowdsource contributors regardless of task and field. His description of the Crowd is Hobbesian, with "a short attention span", producing "mostly crap", although in the end it's worth it, because the Crowd always manages to "find the best stuff" in the end.

Although dense with hyperbole, Howe wasn't far off in his early judgement of the nature of crowdsourcing. Nearly a decade later, while crowdsourcing techniques continue to gain momentum in a variety fields, the fundamental concerns remain the same, with additional ones arising from the changing terrain of technology along with the emergence of new fields, such as the rise of digital humantiies.

It is the diversity of the crowd-- that which gives crowdsourcing projects the ability to shine where traditional approaches have failed (as Howe notes, the odds of a contributor solving a problem actually increases when they have no formal training in that area)-- that also makes both the process and the product hard to judge. Because contributors can come from a wide variety of backgrounds and skillsets, they are often united only by a focused interest in the subject or activity area, whether amateur or professional. Beyond that, motivations vary by particpant-- whether personal or altruistic, extrinsic or intrinsic. In fact, the term crowdsourcing itself can be misleading-- most crowdsourcing projects involve not crowds, which are defined as large anonymous masses of people, but call on the participation of those members of the public with a particular interest and engagement in the task at hand (Dunn and Hedges).

One way to gain insight into the user is to look into the nature of the task itself. In "Crowdsourcing Systems on the World-Wide Web", a 2011 survey paper evaluating the state of crowdsourcing systems (CS) at that time, the author proposes four basic roles in any CS system: the **slave**, **perspective provider**, **content provider**, and **component provider**. The slave works on problems that need many resources but can be divided into simple subtasks per user, such as in Amazon Mechanical Turk; the perspective provider provides opinions (as in book reviews), the content provider produces her own material, whether artistic or scientific, and the compenent providers create a community or network. However, humans can play multiple roles in a single CS system (Dohan et al.).

Attempts to formulate typologies, or the marking of a system into essential parts and processes, have been prevalent in the research of crowdsourcing and similiar activities. Most specifities thus far have focused on the realm of **crowd science**, also known as **citizen science**, **networked science**, or **massively-collaborative science** (naming proves often to be a science within itself). Crowd science is marked by open participation and the transparency of intermediary steps, to varying degrees (Franzoni and Sauermann).

One popular typology for crowd science, presented by Bonney et al., creates three tiers of projects, distinguished by the level of control contributors have in the overall design. These divisions are **contributory**, **collaborative**, and **co-created projects** (Dunn and Hedges). This task-based approach is useful in defining crowd science, as the classification of the project in turn helps classify its users, which remain the core of all crowdsourcing-related problems. The four fundamental challenges of crowdsourcing systems, as detailed by Dohan et al., are all focused on the care and keeping of the user: how to recruit the user, how to retain the user, how to combine the works of different users, and how to evaluate these user contributions. Even in the most unexpected of use-cases, such as when Jerome Lewis, a social anthropologist worked with Rwandan pygmy hunter-gatherers to target poachers and loggers with specially designed smartphones, defining the type of task makes data gathering and collaboration easier (Gura).

These typologies for crowd science are helpful, but must also be adjusted and overwritten for other areas of academia. With the advent of digital research and the digital humanities, the next challenge is to categorize crowdsourcing methods outside of scientific applications. One particular issue lies in the fact that task and process types are far more difficult and also significant for projects in the humanities; since there is so much diveresity in research material (Dunn and Hedges). Dunn and Hedges propose a research infrastructure for the humanitries that takes into account six types of tasks: those that are mechanical, configurational, editorial, synthetic, investigative and creative, in addition to the asset type and the output type. Previous attempts in the Galleries, Libraries, Archives, and Museums (GLAM) sectors to create typologies focus only on task typologies and do not fully reflect the complexity of the project at hand (Dunn and Hedges).

Beyond categorization of processes, there lie questions in quality control universal to all crowdsourcing projects. The issue balancing openess with quality of data collected can be especially tricky to navigate, as crowdsourced projects rely heavily an image of inclusion and interest (the "romance" factor) to gain traction. In reality, ensuring that the data collected is sound-- verifying the crowdsourced data-- becomes extremely difficult when a mix of professionals and amateurs collaborate. Crowdsourcing is a double-edged sword-- the larger the project grows, the harder it becomes to monitor quality, and although projects may be initially appealing due to the low cost of labor, it is in fact very expensive in terms of management. Managing contributors, databases, and directors can be difficult ona large scale, and arguably **less** efficient than traditional setups (Gura). 

Then, there is the question of acceptance-- passing citizen-science data projects through peer review can be challenging, although popularity has grown over the past few years (Gura). Alternative publications-- such as blogposts-- can provide an outlet for such projects, but barriers remain, especially when expanding beyond science.

Still, crowdsourcing, in science, the humanities, and beyond, remains extremely attractive to funders and policy makers. Even if crowd contribution is messy, it is often free, and thus automatically yields high returns given an investment (Franzoni and Sauermann). One might say that the means justify the ends-- and the initial enthusiasm that Howe writes with in the 2006 issue of **Wired** magazine not only lives on, but has grown more and more into an established new workflow. 
